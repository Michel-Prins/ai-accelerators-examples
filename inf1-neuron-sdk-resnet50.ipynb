{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync s3://imagenet-dataset-us-west-2/imagenet-data/tfrecords/validation/ /home/ubuntu/datasets/\n",
    "# !pip install matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/aws/neuron/bin/neuron-cli reset\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.neuron as tfn\n",
    "import tensorflow.compat.v1.keras as keras\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from concurrent import futures\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet50 FP16 saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python gen_resnet50_keras.py\n",
    "!python optimize_for_inference.py --graph resnet50_fp32_keras.pb --out_graph resnet50_fp32_keras_opt.pb\n",
    "!python fp32tofp16.py  --graph resnet50_fp32_keras_opt.pb --out_graph resnet50_fp16_keras_opt.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pb_to_saved_model(pb_path, input_names, output_names, model_dir):\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(open(pb_path, 'rb').read())\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "        inputs = {name: sess.graph.get_tensor_by_name(ts_name) for name, ts_name in input_names.items()}\n",
    "        outputs = {name: sess.graph.get_tensor_by_name(ts_name) for name, ts_name in output_names.items()}\n",
    "        tf.saved_model.simple_save(sess, model_dir, inputs, outputs)\n",
    "\n",
    "saved_model_dir = 'resnet50_saved_model_fp16'\n",
    "pb_to_saved_model(\"resnet50_fp16_keras_opt.pb\", \n",
    "                  {\"input_1:0\": \"input_1:0\"}, \n",
    "                  {\"probs/Softmax:0\" : \"probs/Softmax:0\"}, \n",
    "                  saved_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet50 FP32 saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export SavedModel\n",
    "saved_model_dir = 'resnet50_saved_model_fp32'\n",
    "shutil.rmtree(saved_model_dir, ignore_errors=True)\n",
    "\n",
    "keras.backend.set_learning_phase(0)\n",
    "model = ResNet50(weights='imagenet')\n",
    "tf.saved_model.simple_save(session = keras.backend.get_session(),\n",
    "                           export_dir = saved_model_dir,\n",
    "                           inputs = {'input_1:0': model.inputs[0]},\n",
    "                           outputs = {'probs/Softmax:0': model.outputs[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile models with different batch sizes and cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Batch | nc1 | nc2 | nc4 | nc8 | nc12 | nc16 |\n",
    "| :-: | :-: | :-: | :-: |:-:|:-:|:-:|\n",
    "|1|      1|   1|   1|   2|   2|    2|\n",
    "|2|      1|   1|   0|   1|   2|    2|\n",
    "|3|      1|   1|   1|   1|   1|    1|\n",
    "|4|      1|   1|   0|   0|   1|    0|\n",
    "|5|      1|   1|   0|   0|   1|    0|\n",
    "\n",
    "0 - Failed, 1 - Compiled, 2 - Compiled with static weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_inf1_model(saved_model_dir, batch_size=1, num_cores=1, use_static_weights=False):\n",
    "    print(f'-----------batch size: {batch_size}, num cores: {num_cores}----------')\n",
    "    print('Compiling...')\n",
    "    parent_dir = 'resnet50_inf1_saved_models'\n",
    "    compiled_model_dir = f'resnet50_{saved_model_dir[-4:]}_batch_{batch_size}_inf1_cores_{num_cores}'\n",
    "    inf1_compiled_model_dir = os.path.join(parent_dir, compiled_model_dir)\n",
    "    \n",
    "    shutil.rmtree(inf1_compiled_model_dir, ignore_errors=True)\n",
    "\n",
    "    example_input = np.zeros([batch_size,224,224,3], dtype='float'+saved_model_dir[-2:])\n",
    "\n",
    "    compiler_args = ['--batching_en', '--rematerialization_en', '--spill_dis',\n",
    "                     '--sb_size', str((batch_size + 6)*10),\n",
    "                     '--enable-replication', 'True', '-O2',\n",
    "                     '--verbose','1', '--num-neuroncores', str(num_cores)]\n",
    "    \n",
    "    if use_static_weights:\n",
    "        compiler_args.append('--static-weights')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    compiled_res = tfn.saved_model.compile(model_dir = saved_model_dir,\n",
    "                            model_feed_dict={'input_1:0': example_input},\n",
    "                            new_model_dir = inf1_compiled_model_dir,\n",
    "                            dynamic_batch_size=True,\n",
    "                            compiler_workdir=f'./compiler-workdir/{inf1_compiled_model_dir}',\n",
    "                            compiler_args = compiler_args)\n",
    "    print(f'Compile time: {time.time() - start_time}')\n",
    "    print(inf1_compiled_model_dir)\n",
    "    print(compiled_res)\n",
    "    print('----------- Done! ----------- \\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [{'batch_size':1, 'num_cores':1},\n",
    "           {'batch_size':1, 'num_cores':4},\n",
    "           {'batch_size':1, 'num_cores':16, 'use_static_weights': True},\n",
    "           {'batch_size':2, 'num_cores':16, 'use_static_weights': True},\n",
    "           {'batch_size':5, 'num_cores':1},\n",
    "           {'batch_size':5, 'num_cores':2},\n",
    "           {'batch_size':5, 'num_cores':12}]\n",
    "\n",
    "for opt in options:\n",
    "    compile_inf1_model('resnet50_saved_model_fp32', **opt)\n",
    "    \n",
    "for opt in options:\n",
    "    compile_inf1_model('resnet50_saved_model_fp16', **opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_image_record(record):\n",
    "    feature_map = {'image/encoded': tf.io.FixedLenFeature([], tf.string, ''),\n",
    "                  'image/class/label': tf.io.FixedLenFeature([1], tf.int64, -1),\n",
    "                  'image/class/text': tf.io.FixedLenFeature([], tf.string, '')}\n",
    "    obj = tf.io.parse_single_example(serialized=record, features=feature_map)\n",
    "    imgdata = obj['image/encoded']\n",
    "    label = tf.cast(obj['image/class/label'], tf.int32)   \n",
    "    label_text = tf.cast(obj['image/class/text'], tf.string)   \n",
    "    return imgdata, label, label_text\n",
    "\n",
    "def val_preprocessing(record):\n",
    "    imgdata, label, label_text = deserialize_image_record(record)\n",
    "    label -= 1\n",
    "    image = tf.io.decode_jpeg(imgdata, channels=3, \n",
    "                              fancy_upscaling=False, \n",
    "                              dct_method='INTEGER_FAST')\n",
    "\n",
    "    shape = tf.shape(image)\n",
    "    height = tf.cast(shape[0], tf.float32)\n",
    "    width = tf.cast(shape[1], tf.float32)\n",
    "    side = tf.cast(tf.convert_to_tensor(256, dtype=tf.int32), tf.float32)\n",
    "\n",
    "    scale = tf.cond(tf.greater(height, width),\n",
    "                  lambda: side / width,\n",
    "                  lambda: side / height)\n",
    "    \n",
    "    new_height = tf.cast(tf.math.rint(height * scale), tf.int32)\n",
    "    new_width = tf.cast(tf.math.rint(width * scale), tf.int32)\n",
    "    \n",
    "    image = tf.image.resize(image, [new_height, new_width], method='bicubic')\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 224, 224)\n",
    "    \n",
    "    [image,] = tf.py_function(preprocess_input, [image], [tf.float32])\n",
    "    \n",
    "    return image, label, label_text\n",
    "\n",
    "def get_dataset(batch_size, use_cache=False):\n",
    "    data_dir = '/home/ubuntu/datasets/*'\n",
    "    files = tf.io.gfile.glob(os.path.join(data_dir))\n",
    "    dataset = tf.data.TFRecordDataset(files)\n",
    "    \n",
    "    dataset = dataset.map(map_func=val_preprocessing, num_parallel_calls=8)\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat(count=1)\n",
    "    \n",
    "    if use_cache:\n",
    "        shutil.rmtree('tfdatacache', ignore_errors=True)\n",
    "        os.mkdir('tfdatacache')\n",
    "        dataset = dataset.cache(f'./tfdatacache/imagenet_val')\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single threaded execution: Single core and multi-core pipeline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def inf1_predict_benchmark_single_threaded(neuron_saved_model_name, user_batch_size, use_cache=True, warm_up=10):\n",
    "    print(f'Running model {neuron_saved_model_name}, user_batch_size: {user_batch_size}\\n')\n",
    "\n",
    "    model_inf1 = tf.contrib.predictor.from_saved_model(neuron_saved_model_name)\n",
    "\n",
    "    iter_times = []\n",
    "    pred_labels = []\n",
    "    actual_labels = []\n",
    "    display_threshold = 0\n",
    "    warm_up = 10\n",
    "\n",
    "    ds = get_dataset(user_batch_size, use_cache)\n",
    "\n",
    "    ds_iter = ds.make_initializable_iterator()\n",
    "    ds_next = ds_iter.get_next()\n",
    "    ds_init_op = ds_iter.initializer\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        if use_cache:\n",
    "            sess.run(ds_init_op)\n",
    "            print('\\nCaching dataset ...')\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                while True:\n",
    "                    (validation_ds,label,_) = sess.run(ds_next)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(f'Caching finished: {time.time()-start_time} sec')  \n",
    "\n",
    "        try:\n",
    "            sess.run(ds_init_op)\n",
    "            counter = 0\n",
    "            ipname = list(model_inf1.feed_tensors.keys())[0]\n",
    "            resname = list(model_inf1.fetch_tensors.keys())[0]\n",
    "\n",
    "            while True:\n",
    "                (validation_ds,label,_) = sess.run(ds_next)\n",
    "\n",
    "                model_feed_dict={ipname: validation_ds}\n",
    "\n",
    "                if counter == 0:\n",
    "                    for i in range(warm_up):\n",
    "                        _ = model_inf1(model_feed_dict);                    \n",
    "\n",
    "                start_time = time.time()\n",
    "                inf1_results = model_inf1(model_feed_dict);\n",
    "                iter_times.append(time.time() - start_time)\n",
    "\n",
    "                actual_labels.extend(l for k in label for l in k)\n",
    "                pred_labels.extend(list(np.argmax(inf1_results[resname], axis=1)))\n",
    "\n",
    "                if (counter+1)*user_batch_size >= display_threshold:\n",
    "                    print(f'Images {(counter+1)*user_batch_size}/50000. Average i/s {np.mean(user_batch_size/np.array(iter_times))}')\n",
    "                    display_threshold+=5000\n",
    "\n",
    "                counter+=1\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "    acc_inf1 = np.sum(np.array(actual_labels) == np.array(pred_labels))/len(actual_labels)\n",
    "    iter_times = np.array(iter_times)\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    results['model']                   = [f'{compiled_model_dir}_user_batch_{user_batch_size}_single_core']\n",
    "    results['accuracy']                = [acc_inf1]\n",
    "    results['prediction_time']         = [np.sum(iter_times)]\n",
    "    results['images_per_sec_mean']     = [np.mean(user_batch_size/np.array(iter_times))]\n",
    "    results['latency_per_thread_99th_percentile'] = [np.percentile(iter_times, q=99, interpolation=\"lower\") * 1000]\n",
    "    results['latency_per_thread_mean']            = [np.mean(iter_times) * 1000]\n",
    "    results['latency_per_thread_median']          = [np.median(iter_times) * 1000]\n",
    "    results['latency_per_thread_min']             = [np.min(iter_times) * 1000]\n",
    "    \n",
    "    display(results)\n",
    "    return results, iter_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_cores = 1\n",
    "compiled_model_precision = 'fp16'\n",
    "\n",
    "saved_model_dir = f'resnet50_saved_model_{compiled_model_precision}'\n",
    "parent_dir = 'resnet50_inf1_saved_models'\n",
    "\n",
    "compiled_model_dir = f'resnet50_{compiled_model_precision}_batch_{batch_size}_inf1_cores_{num_cores}'\n",
    "inf1_compiled_model_dir = os.path.join(parent_dir, compiled_model_dir)\n",
    "\n",
    "print(f'inf1_compiled_model_dir: {inf1_compiled_model_dir}')\n",
    "print(f'compiled_model_precision: {compiled_model_precision}')\n",
    "\n",
    "results, latency_per_thread = inf1_predict_benchmark_single_threaded_1(inf1_compiled_model_dir, \n",
    "                                                                     user_batch_size = batch_size*10, \n",
    "                                                                     use_cache=False, \n",
    "                                                                     warm_up=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-threaded execution\n",
    "### Benchmark: Measure using latency of first thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multi-process multi-core batched\n",
    "# def inf1_benchmark_latency_first_threads(neuron_saved_model_name, user_batch_size, num_model_copies, threads_per_model = 1, use_cache=True, warm_up=True):\n",
    "#     try:\n",
    "#         predictor_list = [tf.contrib.predictor.from_saved_model(neuron_saved_model_name) for _ in range(num_model_copies)]\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "\n",
    "#     predictor_list = predictor_list * threads_per_model\n",
    "#     inference_threads = len(predictor_list)\n",
    "        \n",
    "#     global latency_per_thread, counter, total_images_predicted, total_latency, total_throughput\n",
    "#     latency_per_thread = [[] for _ in range(inference_threads)]\n",
    "#     total_images_predicted = [0 for _ in range(inference_threads)]\n",
    "#     total_latency = [0 for _ in range(inference_threads)]\n",
    "#     total_throughput = []\n",
    "#     counter = 0\n",
    "    \n",
    "#     ipname = list(predictor_list[0].feed_tensors.keys())[0]\n",
    "#     resname = list(predictor_list[0].fetch_tensors.keys())[0]\n",
    "\n",
    "#     iter_times = []\n",
    "#     pred_labels = []\n",
    "#     actual_labels = []\n",
    "\n",
    "#     ds = get_dataset(user_batch_size, use_cache)\n",
    "\n",
    "#     ds_iter = ds.make_initializable_iterator()\n",
    "#     ds_next = ds_iter.get_next()\n",
    "#     ds_init_op = ds_iter.initializer\n",
    "    \n",
    "#     if use_cache:\n",
    "#         with tf.Session() as sess:\n",
    "#             sess.run(ds_init_op)\n",
    "#             print('\\nCaching dataset ...')\n",
    "#             start_time = time.time()\n",
    "#             try:\n",
    "#                 while True:\n",
    "#                     (validation_ds,label,_) = sess.run(ds_next)\n",
    "#             except tf.errors.OutOfRangeError:\n",
    "#                 pass\n",
    "#             print(f'Caching finished: {time.time()-start_time} sec')  \n",
    "            \n",
    "#     ds_iter = ds.make_initializable_iterator()\n",
    "#     ds_next = ds_iter.get_next()\n",
    "#     ds_init_op = ds_iter.initializer\n",
    "    \n",
    "#     def inf1_predict(predictor, model_feed_dict, counter, inference_threads, thread_assignment, user_batch_size, warm_up):\n",
    "#         global latency_per_thread, total_images_predicted, total_latency\n",
    "#         if counter in range(inference_threads):\n",
    "#             if warm_up:\n",
    "#                 _ = predictor(model_feed_dict)\n",
    "       \n",
    "#         start_time = time.time()\n",
    "#         pred = predictor(model_feed_dict)\n",
    "#         latency = time.time() - start_time\n",
    "        \n",
    "#         latency_per_thread[thread_assignment].append(latency)\n",
    "#         total_images_predicted[thread_assignment] += user_batch_size\n",
    "#         total_latency[thread_assignment] += latency\n",
    "        \n",
    "#         return {'index':counter,'latency':latency, **pred}\n",
    "    \n",
    "#     def measure_throughput(user_batch_size):\n",
    "#         global counter, total_images_predicted, total_latency, total_throughput, throughput\n",
    "#         display_threshold = 0\n",
    "#         start_time = 0\n",
    "#         throughput = 0\n",
    "#         while counter >= 0:\n",
    "#             if total_latency[0] > 1.0:\n",
    "#                 throughput = sum(total_images_predicted)/total_latency[0]\n",
    "#                 total_throughput.append(throughput)\n",
    "#                 total_images_predicted = [0 for _ in range(len(total_images_predicted))]\n",
    "#                 total_latency = [0 for _ in range(len(total_latency))]\n",
    "#             if (counter)*user_batch_size >= display_threshold:\n",
    "#                 print(f'Images: {counter*user_batch_size}/50000. Average images/sec across threads: {throughput}')\n",
    "#                 display_threshold+=5000\n",
    "\n",
    "#     # submit each image to predictors in a round-robin fashion\n",
    "#     future_list = []\n",
    "#     with futures.ThreadPoolExecutor(max_workers=inference_threads + 1) as executor:\n",
    "#         executor.submit(measure_throughput, user_batch_size)\n",
    "#         inference_walltime = time.time()\n",
    "#         with tf.Session() as sess:\n",
    "#             try:\n",
    "#                 sess.run(ds_init_op)\n",
    "#                 while True:\n",
    "#                     (validation_ds,label,_) = sess.run(ds_next)\n",
    "#                     model_feed_dict={ipname: validation_ds}\n",
    "#                     actual_labels.extend(l for k in label for l in k)\n",
    "                    \n",
    "#                     thread_assignment = counter % len(predictor_list)\n",
    "#                     predictor = predictor_list[thread_assignment]\n",
    "#                     ex_results = executor.submit(inf1_predict, predictor, model_feed_dict, counter, inference_threads, thread_assignment, user_batch_size, warm_up)\n",
    "                    \n",
    "#                     future_list.append(ex_results)\n",
    "#                     counter+=1\n",
    "\n",
    "#             except tf.errors.OutOfRangeError:\n",
    "#                 counter = -1\n",
    "#                 pass\n",
    "\n",
    "#         pred_labels = []\n",
    "#         img_index = []\n",
    "#         iter_times_all_threads = []\n",
    "#         for f in future_list:\n",
    "#             res = f.result()\n",
    "#             img_index.append(res['index'])\n",
    "#             iter_times_all_threads.append(res['latency'])\n",
    "#             pred_labels.extend(np.argmax(res['probs/Softmax:0'], axis=1))\n",
    "#         inference_walltime = time.time() - inference_walltime\n",
    "        \n",
    "#     iter_times_all_threads = np.array(iter_times_all_threads)\n",
    "#     acc_inf1 = np.sum(np.array(actual_labels) == np.array(pred_labels))/len(actual_labels)\n",
    "    \n",
    "#     results = pd.DataFrame()\n",
    "#     results['model']                              = [neuron_saved_model_name]\n",
    "#     results['instance_type']                      = [requests.get('http://169.254.169.254/latest/meta-data/instance-type').text]\n",
    "#     results['user_batch_size']                    = [user_batch_size]\n",
    "#     results['num_model_copies']                   = [num_model_copies]\n",
    "#     results['threads_per_model']                  = [threads_per_model]\n",
    "#     results['accuracy']                           = [acc_inf1]\n",
    "#     results['prediction_time']                    = [np.sum(iter_times_all_threads)]\n",
    "#     results['images_per_sec_mean']                = [np.mean(total_throughput)]\n",
    "#     results['images_per_sec_per_thread_mean']     = [np.mean(user_batch_size / iter_times_all_threads)]\n",
    "#     results['latency_per_thread_99th_percentile'] = [np.percentile(iter_times_all_threads, q=99, interpolation=\"lower\") * 1000]\n",
    "#     results['latency_per_thread_mean']            = [np.mean(iter_times_all_threads) * 1000]\n",
    "#     results['latency_per_thread_median']          = [np.median(iter_times_all_threads) * 1000]\n",
    "#     results['latency_per_thread_min']             = [np.min(iter_times_all_threads) * 1000]\n",
    "#     results['inference_walltime']                 = [inference_walltime]\n",
    "\n",
    "#     display(results)\n",
    "#     return results, latency_per_thread, iter_times_all_threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-threaded execution\n",
    "### Benchmark: Measure using max latency thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate images per 1 sec, Multi-process multi-core batched\n",
    "def inf1_benchmark_latency_max_threads(neuron_saved_model_name, user_batch_size, num_model_copies, threads_per_model = 1, use_cache=True, warm_up=True):\n",
    "    \n",
    "    try:\n",
    "        predictor_list = [tf.contrib.predictor.from_saved_model(neuron_saved_model_name) for _ in range(num_model_copies)]\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    predictor_list = predictor_list * threads_per_model\n",
    "    inference_threads = len(predictor_list)\n",
    "        \n",
    "    global latency_per_thread, counter, total_images_predicted, total_latency, total_throughput\n",
    "    latency_per_thread = [[] for _ in range(inference_threads)]\n",
    "    total_images_predicted = [0 for _ in range(inference_threads)]\n",
    "    total_latency = [0 for _ in range(inference_threads)]\n",
    "    total_throughput = []\n",
    "    counter = 0\n",
    "    \n",
    "    ipname = list(predictor_list[0].feed_tensors.keys())[0]\n",
    "    resname = list(predictor_list[0].fetch_tensors.keys())[0]\n",
    "\n",
    "    iter_times = []\n",
    "    pred_labels = []\n",
    "    actual_labels = []\n",
    "\n",
    "    ds = get_dataset(user_batch_size, use_cache)\n",
    "\n",
    "    ds_iter = ds.make_initializable_iterator()\n",
    "    ds_next = ds_iter.get_next()\n",
    "    ds_init_op = ds_iter.initializer\n",
    "    \n",
    "    if use_cache:\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(ds_init_op)\n",
    "            print('\\nCaching dataset ...')\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                while True:\n",
    "                    (validation_ds,label,_) = sess.run(ds_next)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            print(f'Caching finished: {time.time()-start_time} sec')  \n",
    "            \n",
    "    ds_iter = ds.make_initializable_iterator()\n",
    "    ds_next = ds_iter.get_next()\n",
    "    ds_init_op = ds_iter.initializer\n",
    "    \n",
    "    def inf1_predict(predictor, model_feed_dict, counter, inference_threads, thread_assignment, user_batch_size, warm_up):\n",
    "        global latency_per_thread, total_images_predicted, total_latency\n",
    "        if counter in range(inference_threads):\n",
    "            if warm_up:\n",
    "                _ = predictor(model_feed_dict)\n",
    "       \n",
    "        start_time = time.time()\n",
    "        pred = predictor(model_feed_dict)\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        latency_per_thread[thread_assignment].append(latency)\n",
    "        total_images_predicted[thread_assignment] += user_batch_size\n",
    "        total_latency[thread_assignment] += latency\n",
    "        \n",
    "        return {'index':counter,'latency':latency, **pred}\n",
    "    \n",
    "    def measure_throughput(user_batch_size):\n",
    "        global counter, total_images_predicted, total_latency, total_throughput, throughput\n",
    "        display_threshold = 0\n",
    "        start_time = 0\n",
    "        throughput = 0\n",
    "        while counter >= 0:\n",
    "            if max(total_latency) -  start_time > 0.1:\n",
    "                throughput = 10*sum(total_images_predicted)/max(total_latency)\n",
    "                total_throughput.append(throughput)\n",
    "                start_time = 0\n",
    "                total_images_predicted = [0 for _ in range(len(total_images_predicted))]\n",
    "                total_latency = [0 for _ in range(len(total_latency))]\n",
    "            if (counter)*user_batch_size >= display_threshold:\n",
    "                print(f'Images: {counter*user_batch_size}/50000. Average images/sec across threads: {throughput}')\n",
    "                display_threshold+=5000\n",
    "\n",
    "    # submit each image to predictors in a round-robin fashion\n",
    "    future_list = []\n",
    "    with futures.ThreadPoolExecutor(max_workers=inference_threads + 1) as executor:\n",
    "        executor.submit(measure_throughput, user_batch_size)\n",
    "        with tf.Session() as sess:\n",
    "            try:\n",
    "                sess.run(ds_init_op)\n",
    "                while True:\n",
    "                    (validation_ds,label,_) = sess.run(ds_next)\n",
    "                    model_feed_dict={ipname: validation_ds}\n",
    "                    actual_labels.extend(l for k in label for l in k)\n",
    "                    \n",
    "                    thread_assignment = counter % len(predictor_list)\n",
    "                    predictor = predictor_list[thread_assignment]\n",
    "                    ex_results = executor.submit(inf1_predict, predictor, model_feed_dict, counter, inference_threads, thread_assignment, user_batch_size, warm_up)\n",
    "                    \n",
    "                    future_list.append(ex_results)\n",
    "                    counter+=1\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                counter = -1\n",
    "                pass\n",
    "\n",
    "        pred_labels = []\n",
    "        img_index = []\n",
    "        iter_times_all_threads = []\n",
    "        for f in future_list:\n",
    "            res = f.result()\n",
    "            img_index.append(res['index'])\n",
    "            iter_times_all_threads.append(res['latency'])\n",
    "            pred_labels.extend(np.argmax(res['probs/Softmax:0'], axis=1))\n",
    "    \n",
    "    iter_times_all_threads = np.array(iter_times_all_threads)\n",
    "    acc_inf1 = np.sum(np.array(actual_labels) == np.array(pred_labels))/len(actual_labels)\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    results['model']                              = [neuron_saved_model_name]\n",
    "    results['instance_type']                      = [requests.get('http://169.254.169.254/latest/meta-data/instance-type').text]\n",
    "    results['user_batch_size']                    = [user_batch_size]\n",
    "    results['num_model_copies']                   = [num_model_copies]\n",
    "    results['threads_per_model']                  = [threads_per_model]\n",
    "    results['accuracy']                           = [acc_inf1]\n",
    "    results['prediction_time']                    = [np.sum(iter_times_all_threads)]\n",
    "    results['images_per_sec_mean']                = [np.mean(total_throughput)]\n",
    "    results['images_per_sec_per_thread_mean']     = [np.mean(user_batch_size / iter_times_all_threads)]\n",
    "    results['latency_per_thread_99th_percentile'] = [np.percentile(iter_times_all_threads, q=99, interpolation=\"lower\") * 1000]\n",
    "    results['latency_per_thread_mean']            = [np.mean(iter_times_all_threads) * 1000]\n",
    "    results['latency_per_thread_median']          = [np.median(iter_times_all_threads) * 1000]\n",
    "    results['latency_per_thread_min']             = [np.min(iter_times_all_threads) * 1000]\n",
    "    display(results)\n",
    "    return results, latency_per_thread, iter_times_all_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_inference_request(batch_size, compiled_num_cores, saved_model_precision, num_model_copies=0, threads_per_model=2):\n",
    "    avail_neuroncores_dict = {\n",
    "        'inf1.xlarge' : 4,\n",
    "        'inf1.2xlarge' : 4,\n",
    "        'inf1.6xlarge' : 16,\n",
    "        'inf1.24xlarge' : 64\n",
    "    }\n",
    "    instance_type = requests.get('http://169.254.169.254/latest/meta-data/instance-type').text\n",
    "    avail_num_cores = avail_neuroncores_dict.get(instance_type, 0)\n",
    "    \n",
    "    if not num_model_copies:\n",
    "        num_model_copies = avail_num_cores//compiled_num_cores\n",
    "    \n",
    "    user_batch_size = batch_size*10\n",
    "\n",
    "    parent_dir = 'resnet50_inf1_saved_models'\n",
    "    compiled_model_dir = f'resnet50_{saved_model_precision}_batch_{batch_size}_inf1_cores_{compiled_num_cores}'\n",
    "    inf1_compiled_model_dir = os.path.join(parent_dir, compiled_model_dir)\n",
    "\n",
    "    print(f\"\"\"\n",
    "-> Compiled batch size: {batch_size}\n",
    "-> Compiled neuron cores: {compiled_num_cores}\n",
    "-> Saved model precision: {saved_model_precision}\n",
    "\n",
    "-> Available neuron cores: {avail_num_cores}\n",
    "-> Number of model copies that fits on {instance_type}: {num_model_copies}\n",
    "-> Number of CPU threads to feed each model: {threads_per_model}\n",
    "-> User batch size: {user_batch_size}\n",
    "\n",
    "-> Compiled model dir: {inf1_compiled_model_dir}\n",
    "            \"\"\")\n",
    "    \n",
    "    results, _, _ = inf1_benchmark_latency_max_threads(inf1_compiled_model_dir, \n",
    "                                                          user_batch_size = user_batch_size, \n",
    "                                                          num_model_copies = num_model_copies, \n",
    "                                                          threads_per_model = threads_per_model,\n",
    "                                                          use_cache=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Compiled batch size: 5\n",
      "-> Compiled neuron cores: 1\n",
      "-> Saved model precision: fp32\n",
      "\n",
      "-> Available neuron cores: 16\n",
      "-> Number of model copies that fits on inf1.6xlarge: 16\n",
      "-> Number of CPU threads to feed each model: 1\n",
      "-> User batch size: 50\n",
      "\n",
      "-> Compiled model dir: resnet50_inf1_saved_models/resnet50_fp32_batch_5_inf1_cores_1\n",
      "            \n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "\n",
      "Caching dataset ...\n",
      "Caching finished: 94.37930631637573 sec\n",
      "Images: 0/50000. Average images/sec across threads: 0\n",
      "Images: 5000/50000. Average images/sec across threads: 2673.4656413812795\n",
      "Images: 10000/50000. Average images/sec across threads: 2789.270399965951\n",
      "Images: 15000/50000. Average images/sec across threads: 2827.363987628988\n",
      "Images: 20000/50000. Average images/sec across threads: 2642.1511624244545\n",
      "Images: 25000/50000. Average images/sec across threads: 2805.9824508353113\n",
      "Images: 30000/50000. Average images/sec across threads: 2757.726861495087\n",
      "Images: 35000/50000. Average images/sec across threads: 2782.3207410493246\n",
      "Images: 40000/50000. Average images/sec across threads: 2806.192712723964\n",
      "Images: 45000/50000. Average images/sec across threads: 2586.324183399888\n",
      "Images: 50000/50000. Average images/sec across threads: 2632.5063987738454\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>user_batch_size</th>\n",
       "      <th>num_model_copies</th>\n",
       "      <th>threads_per_model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>prediction_time</th>\n",
       "      <th>images_per_sec_mean</th>\n",
       "      <th>images_per_sec_per_thread_mean</th>\n",
       "      <th>latency_per_thread_99th_percentile</th>\n",
       "      <th>latency_per_thread_mean</th>\n",
       "      <th>latency_per_thread_median</th>\n",
       "      <th>latency_per_thread_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet50_inf1_saved_models/resnet50_fp32_batch...</td>\n",
       "      <td>inf1.6xlarge</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.74862</td>\n",
       "      <td>214.939892</td>\n",
       "      <td>2656.106881</td>\n",
       "      <td>265.068709</td>\n",
       "      <td>1117.420673</td>\n",
       "      <td>214.939892</td>\n",
       "      <td>182.432055</td>\n",
       "      <td>171.118975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model instance_type  \\\n",
       "0  resnet50_inf1_saved_models/resnet50_fp32_batch...  inf1.6xlarge   \n",
       "\n",
       "   user_batch_size  num_model_copies  threads_per_model  accuracy  \\\n",
       "0               50                16                  1   0.74862   \n",
       "\n",
       "   prediction_time  images_per_sec_mean  images_per_sec_per_thread_mean  \\\n",
       "0       214.939892          2656.106881                      265.068709   \n",
       "\n",
       "   latency_per_thread_99th_percentile  latency_per_thread_mean  \\\n",
       "0                         1117.420673               214.939892   \n",
       "\n",
       "   latency_per_thread_median  latency_per_thread_min  \n",
       "0                 182.432055              171.118975  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Compiled batch size: 5\n",
      "-> Compiled neuron cores: 1\n",
      "-> Saved model precision: fp32\n",
      "\n",
      "-> Available neuron cores: 16\n",
      "-> Number of model copies that fits on inf1.6xlarge: 16\n",
      "-> Number of CPU threads to feed each model: 2\n",
      "-> User batch size: 50\n",
      "\n",
      "-> Compiled model dir: resnet50_inf1_saved_models/resnet50_fp32_batch_5_inf1_cores_1\n",
      "            \n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "\n",
      "Caching dataset ...\n",
      "Caching finished: 131.82000708580017 sec\n",
      "Images: 0/50000. Average images/sec across threads: 0\n",
      "Images: 5000/50000. Average images/sec across threads: 2796.2324931465937\n",
      "Images: 10000/50000. Average images/sec across threads: 2658.4388223094925\n",
      "Images: 15000/50000. Average images/sec across threads: 2646.281972012265\n",
      "Images: 20000/50000. Average images/sec across threads: 2512.790097185328\n",
      "Images: 25000/50000. Average images/sec across threads: 789.5689574680205\n",
      "Images: 30000/50000. Average images/sec across threads: 1071.0495652655763\n",
      "Images: 35000/50000. Average images/sec across threads: 708.75613695407\n",
      "Images: 40000/50000. Average images/sec across threads: 300.3172076447744\n",
      "Images: 45000/50000. Average images/sec across threads: 582.2051901066161\n",
      "Images: 50000/50000. Average images/sec across threads: 787.0655877981775\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>user_batch_size</th>\n",
       "      <th>num_model_copies</th>\n",
       "      <th>threads_per_model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>prediction_time</th>\n",
       "      <th>images_per_sec_mean</th>\n",
       "      <th>images_per_sec_per_thread_mean</th>\n",
       "      <th>latency_per_thread_99th_percentile</th>\n",
       "      <th>latency_per_thread_mean</th>\n",
       "      <th>latency_per_thread_median</th>\n",
       "      <th>latency_per_thread_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet50_inf1_saved_models/resnet50_fp32_batch...</td>\n",
       "      <td>inf1.6xlarge</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.74862</td>\n",
       "      <td>685.417934</td>\n",
       "      <td>1484.43441</td>\n",
       "      <td>142.486072</td>\n",
       "      <td>2080.97291</td>\n",
       "      <td>685.417934</td>\n",
       "      <td>544.714093</td>\n",
       "      <td>173.460007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model instance_type  \\\n",
       "0  resnet50_inf1_saved_models/resnet50_fp32_batch...  inf1.6xlarge   \n",
       "\n",
       "   user_batch_size  num_model_copies  threads_per_model  accuracy  \\\n",
       "0               50                16                  2   0.74862   \n",
       "\n",
       "   prediction_time  images_per_sec_mean  images_per_sec_per_thread_mean  \\\n",
       "0       685.417934           1484.43441                      142.486072   \n",
       "\n",
       "   latency_per_thread_99th_percentile  latency_per_thread_mean  \\\n",
       "0                          2080.97291               685.417934   \n",
       "\n",
       "   latency_per_thread_median  latency_per_thread_min  \n",
       "0                 544.714093              173.460007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Compiled batch size: 5\n",
      "-> Compiled neuron cores: 1\n",
      "-> Saved model precision: fp32\n",
      "\n",
      "-> Available neuron cores: 16\n",
      "-> Number of model copies that fits on inf1.6xlarge: 16\n",
      "-> Number of CPU threads to feed each model: 4\n",
      "-> User batch size: 50\n",
      "\n",
      "-> Compiled model dir: resnet50_inf1_saved_models/resnet50_fp32_batch_5_inf1_cores_1\n",
      "            \n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:The specified SavedModel has no variables; no checkpoints were restored.\n",
      "\n",
      "Caching dataset ...\n",
      "Caching finished: 101.65202355384827 sec\n",
      "Images: 0/50000. Average images/sec across threads: 0\n",
      "Images: 5000/50000. Average images/sec across threads: 323.7482794882894\n",
      "Images: 10000/50000. Average images/sec across threads: 491.2715432550577\n",
      "Images: 15000/50000. Average images/sec across threads: 311.4492598009929\n",
      "Images: 20000/50000. Average images/sec across threads: 2009.0664888623205\n",
      "Images: 25000/50000. Average images/sec across threads: 513.8425445245862\n",
      "Images: 30000/50000. Average images/sec across threads: 201.25492592589748\n",
      "Images: 35000/50000. Average images/sec across threads: 238.1810510439399\n",
      "Images: 40000/50000. Average images/sec across threads: 2064.9206535192184\n",
      "Images: 45000/50000. Average images/sec across threads: 175.57412638528788\n",
      "Images: 50000/50000. Average images/sec across threads: 185.5256923401932\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>user_batch_size</th>\n",
       "      <th>num_model_copies</th>\n",
       "      <th>threads_per_model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>prediction_time</th>\n",
       "      <th>images_per_sec_mean</th>\n",
       "      <th>images_per_sec_per_thread_mean</th>\n",
       "      <th>latency_per_thread_99th_percentile</th>\n",
       "      <th>latency_per_thread_mean</th>\n",
       "      <th>latency_per_thread_median</th>\n",
       "      <th>latency_per_thread_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet50_inf1_saved_models/resnet50_fp32_batch...</td>\n",
       "      <td>inf1.6xlarge</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.74862</td>\n",
       "      <td>1358.257376</td>\n",
       "      <td>769.428956</td>\n",
       "      <td>78.040777</td>\n",
       "      <td>2826.501369</td>\n",
       "      <td>1358.257376</td>\n",
       "      <td>1413.931966</td>\n",
       "      <td>210.726976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model instance_type  \\\n",
       "0  resnet50_inf1_saved_models/resnet50_fp32_batch...  inf1.6xlarge   \n",
       "\n",
       "   user_batch_size  num_model_copies  threads_per_model  accuracy  \\\n",
       "0               50                16                  4   0.74862   \n",
       "\n",
       "   prediction_time  images_per_sec_mean  images_per_sec_per_thread_mean  \\\n",
       "0      1358.257376           769.428956                       78.040777   \n",
       "\n",
       "   latency_per_thread_99th_percentile  latency_per_thread_mean  \\\n",
       "0                         2826.501369              1358.257376   \n",
       "\n",
       "   latency_per_thread_median  latency_per_thread_min  \n",
       "0                1413.931966              210.726976  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>instance_type</th>\n",
       "      <th>user_batch_size</th>\n",
       "      <th>num_model_copies</th>\n",
       "      <th>threads_per_model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>prediction_time</th>\n",
       "      <th>images_per_sec_mean</th>\n",
       "      <th>images_per_sec_per_thread_mean</th>\n",
       "      <th>latency_per_thread_99th_percentile</th>\n",
       "      <th>latency_per_thread_mean</th>\n",
       "      <th>latency_per_thread_median</th>\n",
       "      <th>latency_per_thread_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet50_inf1_saved_models/resnet50_fp32_batch...</td>\n",
       "      <td>inf1.6xlarge</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.74862</td>\n",
       "      <td>214.939892</td>\n",
       "      <td>2656.106881</td>\n",
       "      <td>265.068709</td>\n",
       "      <td>1117.420673</td>\n",
       "      <td>214.939892</td>\n",
       "      <td>182.432055</td>\n",
       "      <td>171.118975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet50_inf1_saved_models/resnet50_fp32_batch...</td>\n",
       "      <td>inf1.6xlarge</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.74862</td>\n",
       "      <td>685.417934</td>\n",
       "      <td>1484.434410</td>\n",
       "      <td>142.486072</td>\n",
       "      <td>2080.972910</td>\n",
       "      <td>685.417934</td>\n",
       "      <td>544.714093</td>\n",
       "      <td>173.460007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet50_inf1_saved_models/resnet50_fp32_batch...</td>\n",
       "      <td>inf1.6xlarge</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.74862</td>\n",
       "      <td>1358.257376</td>\n",
       "      <td>769.428956</td>\n",
       "      <td>78.040777</td>\n",
       "      <td>2826.501369</td>\n",
       "      <td>1358.257376</td>\n",
       "      <td>1413.931966</td>\n",
       "      <td>210.726976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model instance_type  \\\n",
       "0  resnet50_inf1_saved_models/resnet50_fp32_batch...  inf1.6xlarge   \n",
       "1  resnet50_inf1_saved_models/resnet50_fp32_batch...  inf1.6xlarge   \n",
       "2  resnet50_inf1_saved_models/resnet50_fp32_batch...  inf1.6xlarge   \n",
       "\n",
       "   user_batch_size  num_model_copies  threads_per_model  accuracy  \\\n",
       "0               50                16                  1   0.74862   \n",
       "1               50                16                  2   0.74862   \n",
       "2               50                16                  4   0.74862   \n",
       "\n",
       "   prediction_time  images_per_sec_mean  images_per_sec_per_thread_mean  \\\n",
       "0       214.939892          2656.106881                      265.068709   \n",
       "1       685.417934          1484.434410                      142.486072   \n",
       "2      1358.257376           769.428956                       78.040777   \n",
       "\n",
       "   latency_per_thread_99th_percentile  latency_per_thread_mean  \\\n",
       "0                         1117.420673               214.939892   \n",
       "1                         2080.972910               685.417934   \n",
       "2                         2826.501369              1358.257376   \n",
       "\n",
       "   latency_per_thread_median  latency_per_thread_min  \n",
       "0                 182.432055              171.118975  \n",
       "1                 544.714093              173.460007  \n",
       "2                1413.931966              210.726976  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "model_list = [{'batch_size':5, 'compiled_num_cores':1, 'saved_model_precision':'fp32', 'threads_per_model':1},\n",
    "             {'batch_size':5, 'compiled_num_cores':1, 'saved_model_precision':'fp32', 'threads_per_model':2},\n",
    "             {'batch_size':5, 'compiled_num_cores':1, 'saved_model_precision':'fp32', 'threads_per_model':4}]\n",
    "\n",
    "for m in model_list:\n",
    "    res = submit_inference_request(**m)\n",
    "    if results.empty:\n",
    "        results = res\n",
    "    else:\n",
    "        results = results.append(res)\n",
    "\n",
    "results = results.reset_index(drop=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "\n",
    "model_list = [{'batch_size':5, 'compiled_num_cores':1, 'saved_model_precision':'fp16', 'threads_per_model':1},\n",
    "             {'batch_size':5, 'compiled_num_cores':1, 'saved_model_precision':'fp16', 'threads_per_model':2},\n",
    "                          {'batch_size':5, 'compiled_num_cores':1, 'saved_model_precision':'fp16', 'threads_per_model':4},\n",
    "                          {'batch_size':5, 'compiled_num_cores':1, 'saved_model_precision':'fp16', 'threads_per_model':8}]\n",
    "\n",
    "for m in model_list:\n",
    "    res = submit_inference_request(**m)\n",
    "    if results.empty:\n",
    "        results = res\n",
    "    else:\n",
    "        results = results.append(res)\n",
    "\n",
    "results = results.reset_index(drop=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_index, iter_times_per_thread, pred_prob = map(list,zip(*[(f.result()['i_num'],f.result()['latency'],f.result()['probs/Softmax:0']) for f in future_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "pred_labels = []\n",
    "img_index = []\n",
    "iter_times_per_thread = []\n",
    "\n",
    "for f in future_list:\n",
    "    res = f.result()\n",
    "    img_index.append(res['i_num'])\n",
    "    iter_times_per_thread.append(res['latency'])\n",
    "    pred_labels.extend(np.argmax(res['probs/Softmax:0'], axis=1))\n",
    "\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(latency_per_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "compiled_num_cores = 1\n",
    "compiled_model_precision = 'fp32'\n",
    "\n",
    "infer_num_cores = 1\n",
    "threads_per_model = 6\n",
    "\n",
    "saved_model_dir = f'resnet50_saved_model_{compiled_model_precision}'\n",
    "parent_dir = 'resnet50_inf1_saved_models'\n",
    "\n",
    "compiled_model_dir = f'resnet50_{compiled_model_precision}_batch_{batch_size}_inf1_cores_{compiled_num_cores}'\n",
    "inf1_compiled_model_dir = os.path.join(parent_dir, compiled_model_dir)\n",
    "\n",
    "print(f'inf1_compiled_model_dir: {inf1_compiled_model_dir}')\n",
    "print(f'compiled_model_precision: {compiled_model_precision}')\n",
    "\n",
    "results, latency_per_thread, pred_prob = inf1_predict_benchmark_multi_threaded(inf1_compiled_model_dir, \n",
    "                                      user_batch_size = batch_size*10, \n",
    "                                      infer_num_cores = infer_num_cores, \n",
    "                                      threads_per_model = threads_per_model,\n",
    "                                      use_cache=False, \n",
    "                                      warm_up=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(latency_per_thread[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "num_cores = 1\n",
    "compiled_model_precision = 'fp32'\n",
    "\n",
    "saved_model_dir = f'resnet50_saved_model_{compiled_model_precision}'\n",
    "parent_dir = 'resnet50_inf1_saved_models'\n",
    "\n",
    "compiled_model_dir = f'resnet50_{compiled_model_precision}_batch_{batch_size}_inf1_cores_{num_cores}'\n",
    "inf1_compiled_model_dir = os.path.join(parent_dir, compiled_model_dir)\n",
    "\n",
    "print(f'inf1_compiled_model_dir: {inf1_compiled_model_dir}')\n",
    "print(f'compiled_model_precision: {compiled_model_precision}')\n",
    "\n",
    "results, latency_per_thread = inf1_predict_benchmark_multi_threaded(inf1_compiled_model_dir, \n",
    "                                      user_batch_size = batch_size*10, \n",
    "                                      infer_num_cores = 16, \n",
    "                                      threads_per_model = 1,\n",
    "                                      use_cache=True, \n",
    "                                      warm_up=10)\n",
    "\n",
    "results, latency_per_thread = inf1_predict_benchmark_multi_threaded(inf1_compiled_model_dir, \n",
    "                                      user_batch_size = batch_size*10, \n",
    "                                      infer_num_cores = 16, \n",
    "                                      threads_per_model = 2,\n",
    "                                      use_cache=True, \n",
    "                                      warm_up=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attributes = [{'batch_size':1, 'num_cores':1},\n",
    "                    {'batch_size':5, 'num_cores':1},\n",
    "                    {'batch_size':1, 'num_cores':16, 'use_static_weights': True},\n",
    "                    {'batch_size':2, 'num_cores':16, 'use_static_weights': True},\n",
    "                    {'batch_size':1, 'num_cores':4},\n",
    "                    {'batch_size':5, 'num_cores':2},\n",
    "                    {'batch_size':5, 'num_cores':12}]\n",
    "\n",
    "saved_model_dir = 'resnet50_saved_model_fp16'\n",
    "parent_dir = 'resnet50_inf1_saved_models'\n",
    "    \n",
    "for model in model_attributes:\n",
    "    batch_size = model['batch_size']\n",
    "    num_cores = model['num_cores']\n",
    "    \n",
    "    compiled_model_dir = f'resnet50_{saved_model_dir[-4:]}_batch_{batch_size}_inf1_cores_{num_cores}'\n",
    "    inf1_compiled_model_dir = os.path.join(parent_dir, compiled_model_dir)\n",
    "    \n",
    "    if num_cores == 1:\n",
    "        infer_num_cores = 16\n",
    "    else:\n",
    "        infer_num_cores = 1\n",
    "        \n",
    "    print(inf1_compiled_model_dir)\n",
    "    \n",
    "    inf1_predict_benchmark_multi_threaded(inf1_compiled_model_dir, \n",
    "                                          user_batch_size = batch_size*10, \n",
    "                                          infer_num_cores = infer_num_cores, \n",
    "                                          use_cache=False, \n",
    "                                          warm_up=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boption = {'compiled_batch_size': 1, 'precision': 'auto_bfloat16', 'compiled_num_cores': 1}\n",
    "results, future_list, latency_per_thread = inf1_predict_benchmark_multi_threaded(boption, \n",
    "                                                                                 user_batch_size=50, \n",
    "                                                                                 infer_num_cores = 4, \n",
    "                                                                                 use_cache=False, \n",
    "                                                                                 warm_up=10)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boption = {'compiled_batch_size': 1, 'compiled_num_cores': 1, 'precision': 'fp16'}\n",
    "results, future_list, latency_per_thread = inf1_predict_benchmark_multi_threaded(boption, \n",
    "                                                                                 user_batch_size=50, \n",
    "                                                                                 infer_num_cores = 4, \n",
    "                                                                                 use_cache=False, \n",
    "                                                                                 warm_up=10)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boption = {'compiled_batch_size': 5, 'compiled_num_cores': 1, 'precision': 'fp16'}\n",
    "results, future_list, latency_per_thread = inf1_predict_benchmark_multi_threaded(boption, \n",
    "                                                                                 user_batch_size=50, \n",
    "                                                                                 infer_num_cores = 4, \n",
    "                                                                                 use_cache=False, \n",
    "                                                                                 warm_up=10)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boption = {'compiled_batch_size': 5, 'compiled_num_cores': 1, 'precision': 'fp16'}\n",
    "results, future_list, latency_per_thread = inf1_predict_benchmark_multi_threaded(boption, \n",
    "                                                                                 user_batch_size=50, \n",
    "                                                                                 infer_num_cores = 16, \n",
    "                                                                                 use_cache=False, \n",
    "                                                                                 warm_up=10)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boption = {'compiled_batch_size': 5, 'compiled_num_cores': 1, 'precision': 'fp16'}\n",
    "results, future_list, latency_per_thread = inf1_predict_benchmark_multi_threaded(boption, \n",
    "                                                                                 user_batch_size=50, \n",
    "                                                                                 infer_num_cores = 32, \n",
    "                                                                                 use_cache=False, \n",
    "                                                                                 warm_up=10)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boption = {'compiled_batch_size': 1, 'compiled_num_cores': 16, 'precision': 'fp16'}\n",
    "results, future_list, latency_per_thread = inf1_predict_benchmark_multi_threaded(boption, \n",
    "                                                                                 user_batch_size=50, \n",
    "                                                                                 infer_num_cores = 1, \n",
    "                                                                                 use_cache=False, \n",
    "                                                                                 warm_up=10)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = tf.contrib.predictor.from_saved_model('resnet50_inf1_saved_models/resnet50_fp16_batch_1_cores_1')\n",
    "resname = list(predictor.fetch_tensors.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_index, iter_times_per_thread, pred_prob = map(list,zip(*[(f.result()['i_num'],f.result()['latency'],f.result()[resname]) for f in future_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10/np.mean(iter_times_per_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [q for q in list(np.argmax(p, axis=1)) for p in pred_prob]\n",
    "a = []\n",
    "for p in pred_prob:\n",
    "    a.extend(np.argmax(p, axis=1))\n",
    "len(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(pred_prob, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sorted = np.argsort(img_index)\n",
    "iter_times_sorted = [iter_times_per_thread[i] for i in idx_sorted]\n",
    "pred_prob_sorted = [pred_prob[i] for i in idx_sorted]\n",
    "\n",
    "idx_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/opt/aws/neuron/bin/neuron-cli reset\n",
    "# model_inf1 = tf.contrib.predictor.from_saved_model('resnet50_inf1_saved_models/resnet50_auto_bfloat16_batch_1_cores_1')\n",
    "# model_inf1 = tf.contrib.predictor.from_saved_model('resnet50_inf1_saved_models/resnet50_fp16_batch_1_cores_1')\n",
    "# model_inf1 = tf.contrib.predictor.from_saved_model('resnet50_inf1_saved_models/resnet50_fp16_batch_5_cores_1')\n",
    "# model_inf1 = tf.contrib.predictor.from_saved_model('resnet50_inf1_saved_models/resnet50_fp16_batch_1_cores_16')\n",
    "\n",
    "blist = [{'compiled_batch_size': 1, 'precision': 'auto_bfloat16', 'num_cores': 1},\n",
    "         {'compiled_batch_size': 1, 'precision': 'fp16',  'num_cores': 1},\n",
    "         {'compiled_batch_size': 5, 'precision': 'fp16', 'num_cores': 1},\n",
    "         {'compiled_batch_size': 1, 'precision': 'fp16', 'num_cores': 16},]\n",
    "\n",
    "results, iter_times = inf1_predict_benchmark(blist[3], user_batch_size=10, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = lambda boption: f'inf1_{boption[\"precision\"]}_batchcompiled_{boption[\"compiled_batch_size\"]}_batchuser_{boption[\"user_batch_size\"]}'\n",
    "use_cache = False\n",
    "\n",
    "for boption in blist:\n",
    "    model_path = f'resnet50_inf1_saved_models/resnet50_{boption['precision']}_{boption['compiled_batch_size']}'\n",
    "    dataset = get_dataset(batch_size = boption['user_batch_size'], use_cache)\n",
    "    if use_cache:\n",
    "        print('Start caching ...')\n",
    "        start_time = time.time()\n",
    "        for _ in dataset:\n",
    "            continue\n",
    "        print(f'Caching finished: {time.time()-start_time} sec')\n",
    "    \n",
    "    res, it = inf1_predict_benchmark(dataset, model_path, **boption)\n",
    "    iter_ds = pd.concat([iter_ds, pd.DataFrame(it, columns=[col_name(boption)])], axis=1)\n",
    "    if results.empty:\n",
    "        results = res\n",
    "    else:\n",
    "        results = results.append(res)\n",
    "    display(results)\n",
    "\n",
    "results = results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
